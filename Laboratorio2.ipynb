{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del valle de Guatemala  \n",
    "Dpto. Ciencias de la computacion  \n",
    "Inteligencia Artificial  \n",
    "Alberto Suriano  \n",
    "\n",
    "Laboratorio 2 \n",
    "Andres Quinto - 18288  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 - Preguntas teóricas\n",
    "\n",
    "1. ¿Por qué el modelo de Naive Bayes se le considera \"naive\"?  \n",
    "\n",
    "    - Se le considera \"naive\" debido a que el algoritmo asume que la presencia o ausencia de una característica no está influenciada por la presencia o ausencia de otras características en el conjunto de datos. En el problema de SPAM & HAM podemos verlo reflejado en que trata a todas las palabras de igual forma no importa el orden en el que esten en una oracion, esto, debido a que Naive Bayes trata al lenguaje como una bolsa de palabras y cada mensaje siendo una parte random de lo que esta dentro de la bolsa. (Informacion sacada del video de naive bayes del modulo de esta semana)\n",
    "    [(Naive Bayes, Clearly Explained, 2021)](https://www.youtube.com/watch?v=O2L2Uv9pdDA)   \n",
    "    \n",
    "2. Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda ¿cómo funciona el truco del Kernel para este modelo? (Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase) \n",
    "   \n",
    "    - Lo que trata SVM es mover la data a espacios con mayor dimensiones. Con ello obtenemos dos cosas importantes que son lo que se busca optimizar, un hiperplano y un margen. SVM intenta encontrar un hiperplano que separe de manera óptima las dos clases. El objetivo de la formulación se centra en maximizar el margen entre las clases.  \n",
    "    \n",
    "    La definicion del plano, que en este caso es la linea divisoria que nos ayuda a clasificar esta dada por:  \n",
    "    $$ w^Tx + b = 0  \n",
    "    $$\n",
    "    Donde:\n",
    "    1. w es el vector de pesos.\n",
    "    2. x es el vector de características.\n",
    "    3. b es el sesgo.\n",
    "\n",
    "    La condición de clasificación para un punto de datos es:\n",
    "\n",
    "    $$ y_i = \\text{sign}(w^Tx_i + b)$$\n",
    "\n",
    "    El objetivo es maximizar el margen, sin embargo, para facilitar la optimización, se minimiza  $$ 1/2 ||w||^2$$ sujeto a restricciones que todos los puntos de datos se clasifique correctamente:\n",
    "    $$ y_i(w^Tx_i + b) \\geq 1, \\quad \\text{para todo } i$$\n",
    "\n",
    "    Formulación del Problema de Optimización con Multiplicadores de Lagrange  \n",
    "    \n",
    "    - Multiplicadores de Lagrange: Se utilizan para incorporar estas restricciones en la función objetivo.  \n",
    "    - Función Lagrangiana: Combina la función objetivo y las restricciones, cada una ponderada por un multiplicador de Lagrange.  \n",
    "    - Minimizar la Lagrangiana: Encontramos: $$ w, b, α $$(multiplicadores de Lagrange) que minimicen la función Lagrangiana.  \n",
    "    \n",
    "    Introduciendo los multiplicadores de Lagrange $$( \\alpha_i \\geq 0 )$$ la función Lagrangiana $$( L ) $$ es:  \n",
    "    $$[ L(w, b, \\alpha) = \\frac{1}{2}\\|w\\|^2 - \\sum_{i=1}^n \\alpha_i [y_i(w^Tx_i + b) - 1] ]$$  \n",
    "\n",
    "    Esto es util por que convierte todo nuestro problema en uno de optimización estándar (programación cuadrática), ademas de transformar un problema de optimización con restricciones en uno manejable, permitiendo soluciones óptimas eficientes y aplicables a problemas lineales y no lineales.  \n",
    "    \n",
    "    [(Rogers, S., & Girolami, M, 2016)](https://github.com/wwkenwong/book/blob/master/Simon%20Rogers%2C%20Mark%20Girolami%20A%20First%20Course%20in%20Machine%20Learning.pdf)  \n",
    "\n",
    "3. Investigue sobre Random Forest y responda:  \n",
    "\n",
    "    a. ¿Qué tipo de ensemble learning es este modelo?  \n",
    "        Random Forest es un método de aprendizaje ensamble (ensemble learning), también conocido como aprendizaje por agregación (bagging), que combina la salida de múltiples modelos de aprendizaje automático para mejorar la precisión y la robustez del modelo final. En el caso de Random Forest, los modelos individuales son árboles de decisión.  \n",
    "\n",
    "    b. ¿Cuál es la idea general detrás de Random Forest?  \n",
    "        Conforme la explicación del video del modulo [(¿Qué es Decision Tree y Random Forest?, 2021)](https://www.youtube.com/watch?v=tYPi6qcCQbg&t=1s).  \n",
    "        Utilizaban la analogia de que el random forest era como una democracia, donde cada uno de los datasets que se generaban eran el resultado luego de su democracia y a lo macro eran varios forest. La idea central de Random Forest es combinar múltiples árboles de decisión para obtener un modelo más robusto y preciso.  Este proceso aumenta la diversidad entre los árboles, lo que ayuda a mejorar el rendimiento general y reduce el riesgo de sobreajuste, común en los árboles de decisión individuales.  \n",
    "\n",
    "    c. ¿Por qué se busca baja correlación entre los árboles de Random Forest?  \n",
    "        La baja correlación entre los árboles de Random Forest ayuda a reducir el sesgo y el sobreajuste. Si los árboles de decisión están altamente correlacionados, es probable que aprendan los mismos patrones y, por lo tanto, sean más propensos al sesgo y al sobreajuste.  \n",
    "[(Random Forests, 2001)](https://link.springer.com/article/10.1023/A:1010933404324)  \n",
    "[(¿Qué es Decision Tree y Random Forest?, 2021)](https://www.youtube.com/watch?v=tYPi6qcCQbg&t=1s)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Naive Bayes: Clasificador de Mensajes Ham/Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIMPIEZA DEL DATASET\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset file\n",
    "with open(\"entrenamiento.txt\", \"r\") as file:\n",
    "    dataset = file.read()\n",
    "\n",
    "# Remove special characters\n",
    "clean_dataset = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", dataset)\n",
    "\n",
    "# Convert text to lowercase\n",
    "clean_dataset = clean_dataset.lower()\n",
    "\n",
    "# Split the cleaned dataset by newline character\n",
    "observations = clean_dataset.split(\"\\n\")\n",
    "\n",
    "# Print the observations\n",
    "for observation in observations:\n",
    "    # print(observation)\n",
    "\n",
    "    # Divide el dataset en conjuntos de entrenamiento y prueba\n",
    "    train_set, test_set = train_test_split(observations, test_size=0.2, random_state=42)\n",
    "\n",
    "    # # Imprime los conjuntos de entrenamiento y prueba\n",
    "    # # print(\"Conjunto de entrenamiento:\")\n",
    "    # for observation in train_set:\n",
    "    #     print(observation)\n",
    "\n",
    "    # # print(\"Conjunto de prueba:\")\n",
    "    # for observation in test_set:\n",
    "    #     print(observation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
